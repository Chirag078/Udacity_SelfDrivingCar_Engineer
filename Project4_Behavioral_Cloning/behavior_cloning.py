# -*- coding: utf-8 -*-
# author : Chirag Chauhan
"""Behavior_Cloning_UDACITY_1.   ipynb
Automatically generated by Colaboratory.

Original file is located at 
    https://colab.research.google.com/drive/1qcSIYn-uubHYVhVw0wltJ9mJ_ua3slTg
"""
'''
!git clone https://github.com/Chirag078/Udacity_Track

!ls Udacity_Track -l  
'''
import tensorflow
import csv
import os
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np
from keras.layers import Cropping2D
from keras.layers import MaxPooling2D,Dense,Conv2D,Flatten,BatchNormalization,Lambda,Dropout
from keras.layers import Dropout
from keras.models import Sequential
import cv2
import matplotlib.image as mpimg
from sklearn.utils import shuffle
from zipfile import ZipFile
from keras.optimizers import Adam
import random


# unZip_file function is used to unzip the folder.
# here we need to unzip "Udacity_data.zip" file
def unZip_file(file_name):
    with ZipFile(file_name, 'r') as zip: 
        zip.printdir() 
        print('Extracting all the files now...') 
        zip.extractall() 
        print('Done!') 

# collect_datainfo funtion is used to collect total samples with their values from the driving_log.csv file         
def collect_dataInfo(csv_file):
    samples = []
    i=0
    with open(csv_file) as csvfile:
        reader = csv.reader(csvfile)
        for line in reader:
            if i == 0:                      # ignoring the titles of columns
                i=1
                continue
            samples.append(line)
    return samples

# Extract Folder 
file_name = "Udacity_data.zip"           # path of .zip folder      
csv_file = "./data/driving_log.csv"      # path of .csv file
unZip_file(file_name)
samples = collect_dataInfo(csv_file)     
print("sample = " + str(len(samples)))

# Splitting Data in Training and Validation set
# Training Data = 80%
# Validation Data = 20%
train_samples, validation_samples = train_test_split(samples, test_size=0.2)
print(len(train_samples))
print(len(validation_samples))

# This generator function will generate the data on the fly.
# This generator will generate total 6 Images when it is called
# - 3 Original Images from the left,center and right camera
# - 3 Filped version of original images from the different camera
def generator(samples, batch_size=32):                   
    num_samples = len(samples)
    correction_factor = 0.2
    while 1: 
        shuffle(samples) 
        for offset in range(0, num_samples, batch_size):
            batch_samples = samples[offset:offset+batch_size]
            images = []
            angles = []
            for batch_sample in batch_samples:
                for cam_pos in range(0,3): 
                    name = './data/IMG/'+batch_sample[cam_pos].split('/')[-1]
                    center_image = mpimg.imread(name)
                    center_angle = float(batch_sample[3])              #getting the steering angle measurement
                    images.append(center_image)                        # read the original image
 
                    if(cam_pos==0):                                    # If camera_position = Center 
                        angles.append(center_angle)                    # no need to change in angle
                    elif(cam_pos==1):                                  # if camera_position = left
                        angles.append(center_angle+correction_factor)  # need to add correction factor in the steering angle
                    elif(cam_pos==2):                                  # if camera_position = right
                        angles.append(center_angle-correction_factor)  # need to substract correction factor in the steering angle
                    else:
                        "Do nothing"
                                
                    images.append(cv2.flip(center_image,1))            # flip the same image and steering multiply angle with -1
                    if(cam_pos==0):
                        angles.append(center_angle*-1)
                    elif(cam_pos==1):
                        angles.append((center_angle+correction_factor)*-1)
                    elif(cam_pos==2):
                        angles.append((center_angle-correction_factor)*-1)
                    else:
                        "Do nothing"
        
            X_train = np.array(images)
            y_train = np.array(angles)
            
            yield shuffle(X_train, y_train)

# Batchsize Value
batch_size = 32
# compile and train the model using the generator function
train_generator = generator(train_samples, batch_size=batch_size)
validation_generator = generator(validation_samples, batch_size=batch_size)

# Modified nVIDEA Layer
def nVIDEA():
    model = Sequential()
    # Lambda Layer for normalization of the image
    model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160,320,3)))
  
    # Cropping Layer needed to crop the image
    model.add(Cropping2D(cropping=((70,25),(0,0)))) 
  
    # 5 Conv2D Layer
    model.add(Conv2D(24, (5,5), strides=(2,2), activation='elu'))
    model.add(Conv2D(36, (5,5), strides=(2,2), activation='elu'))
    model.add(Conv2D(48, (5,5), strides=(2,2), activation='elu'))
    model.add(Conv2D(64, (3,3), activation='elu'))
    model.add(Conv2D(64, (3,3), activation='elu'))

    # 1 Dropout Layer with 0.5 drop rate
    model.add(Dropout(0.5))

    # Flatten the data
    model.add(Flatten())
    
    # 4 Fully Connected Layer 
    model.add(Dense(100, activation='elu'))
    model.add(Dense(50, activation='elu'))
    model.add(Dense(10, activation='elu'))
    model.add(Dense(1, activation='elu'))
   
    # Model Parameter 
    model.compile(loss = 'mse',optimizer = 'adam')
    return model

model = nVIDEA()
print(model.summary())

model.fit_generator(train_generator, samples_per_epoch= len(train_samples), 
                    validation_data=validation_generator,   nb_val_samples=len(validation_samples), 
                    nb_epoch=5, verbose=1)

model.save("model.h5")
print("Model Saved")
